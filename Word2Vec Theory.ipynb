{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec model learns word embeddings by modeling each word as vector in n-dimensional space.   \n",
    "But why we use word-embedding? \n",
    "1. Word2Vec creates vector spaced models that represents(embed) words in a continuous vector space.\n",
    "2. With words represented as vectors we can perform vector mathematics on words(eg: check similarity, add/subtract vectors).\n",
    "\n",
    "At the start of training each embedding is random, but through backpropagation the model will adjust the value of each word vector in the given number of dimensions. More dimensions means more training but more information per word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Word2Vec actually creates these word-embeddings and how it learns from these raw text?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec comes in the two flavors. ie Continuous Bag of Words(CBOW) and Skip-Gram Model. Algorithmly, both models are similar expect in the way they end of predicting target word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Continous Bag of word(CBOW):\n",
    " The Bag of words approach takes in source context words such as (The dow chews the ____ ) and then it attempts to find its prediction target (bone). Typically better for smaller datasets. \n",
    " #### Skip-Gram Model: \n",
    " It does exact the inverse. It predicts source context words from the target word. You fed in the word (bone) and it tries to predict (The dow chews the). Typically better for larger datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How it actually trains the model. It does this by something called \"Noise Contrastive Training\".\n",
    "The way it works, if we think about of bag of words approach. Given the \"The dog chews the ____\" we are looking for the word that is going to be the real word target(wt). We know real target is \"bone\". We also have bunch of noise words.\n",
    "![alt text](images/img001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are looking for the word thats most likely to fit there as target. To visualize this it would look something like this.\n",
    "![title](images/img002.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec doesn't actually need a full probabilistic model. Instead we end up doing is we use a binary classification objective such as \"Logistic Regression\" to discriminate the real target words ie (wt) from imaginary noise words we call those (k) in the same context.This is a basically a simple visualization where we've our projection layer [The cat sit on the _____] and target word \"mat\". And we're going to have some sorts of noise classifier. So, its a binary classifier where we compare that target word versus the rest of these noise words then we pass them through some hidden layers and eventually we get some sorts of embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Contrastive Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target word is predicted by maximizing the given equation:\n",
    "    ![title](images/img003.png)\n",
    "    \n",
    "The first term of the equation is the binary logistic regression probability under the model of seeing the word(wt) in the context(h) for some dataset(D) and it is calculated in terms of the learn embedding vector(Theta). So, in practice we approximate the expectation by drawing a constrast of words from noise distribution. Note that: We are only drawing K words. We arenot drawing entire corpus of text data. Instead we're just drawing a certain amount of words from the noise distribution. This really works well for us because it is computationally efficient and we are just computing the lose function and it scales only to the number of noises that we select ie \"K\" number not all the words in the vocab. So thats makes Word2Vec computationally efficient for the task its actually doing. Now, larger amount of K words you end up drawing the longer the training time is going to take but that should make more accurate prediction of target word versus a wide variety of noise words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The goal is to:-  Assign high probability to correct words and low probability to noise words.    \n",
    "Once we have vectors for each word we can visualize realtionships by reducing the dimensions n-dimension to 2-dim using  \"t-Distributed Stochastic Neighbour Embedding\" \n",
    "![title](images/img004.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
